{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert num_hiddens % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        qkv_bias = False\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=qkv_bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=qkv_bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=qkv_bias)\n",
    "        self.W_h = nn.Linear(num_hiddens, num_hiddens)\n",
    "\n",
    "    def dot_product_attention(self, Q, K, V):\n",
    "        # input shape:  (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        # output shape: (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        d = Q.shape[-1]\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / np.sqrt(d)  # (batch_size, num_heads, num_patches, num_patches)\n",
    "        A = nn.Softmax(dim=-1)(scores)\n",
    "        H = torch.matmul(nn.Dropout(self.dropout)(A), V)  # (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        return H\n",
    "    \n",
    "    def split_heads(self, X):\n",
    "        # input:  (batch_size, num_patches, num_hiddens)\n",
    "        # output: (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        return X.reshape(X.shape[0], X.shape[1], self.num_heads, -1).transpose(1, 2)\n",
    "    \n",
    "    def concat_heads(self, X):\n",
    "        # input:  (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        # output: (batch_size, num_patches, num_hiddens)\n",
    "        X = X.transpose(1,2)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input shape:  (batch_size, num_patches, in_hiddens)\n",
    "        # return shape: (batch_size,)\n",
    "        Q = self.split_heads(self.W_q(X))  # (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        K = self.split_heads(self.W_k(X))\n",
    "        V = self.split_heads(self.W_v(X))\n",
    "        H = self.dot_product_attention(Q, K, V)  # (batch_size, num_heads, num_patches, num_hiddens/num_heads)\n",
    "        H = self.W_h(self.concat_heads(H))  # (batch_size, num_patches, num_hiddens)\n",
    "        return H\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads, mlp_hiddens, dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(num_hiddens)\n",
    "        self.attention = MultiHeadSelfAttention(num_hiddens, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(num_hiddens)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_hiddens, mlp_hiddens),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hiddens, num_hiddens),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.attention(self.norm1(X))\n",
    "        X = X + self.mlp(self.norm2(X))\n",
    "        return X\n",
    "    \n",
    "\n",
    "class Predictor(nn.Module): #input shape (11, 2048, 5, 8)\n",
    "    def __init__(self, num_hiddens = 512, num_heads = 8, mlp_hiddens = 2048, dropout = 0.1, in_size = (11, 2048, 20, 30), num_layers = 2):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        T_, C_, H_, W_ = in_size\n",
    "        # self.transformer = nn.Sequential(\n",
    "        #     TransformerBlock(num_hiddens, num_heads, mlp_hiddens, dropout),\n",
    "        #     TransformerBlock(num_hiddens, num_heads, mlp_hiddens, dropout),\n",
    "        #     TransformerBlock(num_hiddens, num_heads, mlp_hiddens, dropout),\n",
    "        # )\n",
    "        self.transformer = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.transformer.add_module(f\"{i}\", TransformerBlock(num_hiddens, num_heads, mlp_hiddens, dropout))\n",
    "        self.linear1 = nn.Linear(C_, self.num_hiddens)\n",
    "        self.linear2 = nn.Linear(self.num_hiddens, C_)\n",
    "        self.linear3 = nn.Linear(T_*H_*W_, H_*W_)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pos_embedding = nn.Parameter(0.02 * torch.randn(1, T_*H_*W_, num_hiddens))\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, T, C, H, W = X.shape\n",
    "        # B * 11 * 2048 * 5 * 8 to B * 440 * 2048\n",
    "        X = X.permute(0,2,1,3,4).reshape(B, C, T*H*W).permute(0,2,1)\n",
    "\n",
    "        # B * 440 * 2048 to B * 440 * 512\n",
    "        X = self.linear1(X)\n",
    "        #add positional embedding\n",
    "        X = X + self.pos_embedding\n",
    "        X = self.transformer(X)\n",
    "\n",
    "        # B * 440 * 512 to B * 440 * 2048\n",
    "        X = self.linear2(X)\n",
    "        X = self.relu(X)\n",
    "        # B * 440 * 2048 to B * 2048 * 40\n",
    "        X = X.permute(0,2,1)\n",
    "        X = self.linear3(X)\n",
    "        X = X.reshape(B, C, H, W)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class VideoInstanceSeg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        fcn_resnet = models.segmentation.fcn_resnet50(num_classes=49, aux_loss=True)\n",
    "        self.backbone = fcn_resnet.backbone\n",
    "        self.predictor = Predictor()\n",
    "        self.classifier = fcn_resnet.classifier\n",
    "        self.aux_classifier = fcn_resnet.aux_classifier\n",
    "        self.conv_t_1 = nn.ConvTranspose2d(49, 49, kernel_size=16, padding=4, stride=8)\n",
    "        self.conv_t_2 = nn.ConvTranspose2d(49, 49, kernel_size=16, padding=4, stride=8)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(11, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.backbone(x)         \n",
    "        x1 = x['out']  # (B * 11, 2048, 20, 30)\n",
    "        x2 = x['aux']  # (B * 11, 1024, 20, 30)\n",
    "\n",
    "        BT, C, H, W = x1.shape\n",
    "        B = BT // 11\n",
    "        x1 = x1.reshape(B, 11, C, H, W)\n",
    "        x1 = self.predictor(x1)  # (B, 2048, 20, 30)\n",
    "\n",
    "        x2 = self.fc(x2.reshape(B, 11, 1024, H, W).permute(0,2,3,4,1)).squeeze(-1)  # (B, 1024, 20, 30)\n",
    "        # -----------------------------\n",
    "        x1 = self.classifier(x1)      # (B, 49, 20, 30)\n",
    "        x2 = self.aux_classifier(x2)  # (B, 49, 20, 30)\n",
    "\n",
    "        y = {}\n",
    "        y['out'] = self.conv_t_1(x1)\n",
    "        y['aux'] = self.conv_t_2(x2)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = VideoInstanceSeg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_resnet = models.segmentation.fcn_resnet50(num_classes=49, aux_loss=True)\n",
    "backbone = fcn_resnet.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1, 3, 160, 240))\n",
    "y = backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 20, 30])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 20, 30])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['aux'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis/anaconda3/envs/dl/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/Users/louis/anaconda3/envs/dl/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "VideoInstanceSeg                                   [1, 49, 160, 240]         --\n",
       "├─IntermediateLayerGetter: 1-1                     [11, 2048, 20, 30]        --\n",
       "│    └─Conv2d: 2-1                                 [11, 64, 80, 120]         9,408\n",
       "│    └─BatchNorm2d: 2-2                            [11, 64, 80, 120]         128\n",
       "│    └─ReLU: 2-3                                   [11, 64, 80, 120]         --\n",
       "│    └─MaxPool2d: 2-4                              [11, 64, 40, 60]          --\n",
       "│    └─Sequential: 2-5                             [11, 256, 40, 60]         --\n",
       "│    │    └─Bottleneck: 3-1                        [11, 256, 40, 60]         75,008\n",
       "│    │    └─Bottleneck: 3-2                        [11, 256, 40, 60]         70,400\n",
       "│    │    └─Bottleneck: 3-3                        [11, 256, 40, 60]         70,400\n",
       "│    └─Sequential: 2-6                             [11, 512, 20, 30]         --\n",
       "│    │    └─Bottleneck: 3-4                        [11, 512, 20, 30]         379,392\n",
       "│    │    └─Bottleneck: 3-5                        [11, 512, 20, 30]         280,064\n",
       "│    │    └─Bottleneck: 3-6                        [11, 512, 20, 30]         280,064\n",
       "│    │    └─Bottleneck: 3-7                        [11, 512, 20, 30]         280,064\n",
       "│    └─Sequential: 2-7                             [11, 1024, 20, 30]        --\n",
       "│    │    └─Bottleneck: 3-8                        [11, 1024, 20, 30]        1,512,448\n",
       "│    │    └─Bottleneck: 3-9                        [11, 1024, 20, 30]        1,117,184\n",
       "│    │    └─Bottleneck: 3-10                       [11, 1024, 20, 30]        1,117,184\n",
       "│    │    └─Bottleneck: 3-11                       [11, 1024, 20, 30]        1,117,184\n",
       "│    │    └─Bottleneck: 3-12                       [11, 1024, 20, 30]        1,117,184\n",
       "│    │    └─Bottleneck: 3-13                       [11, 1024, 20, 30]        1,117,184\n",
       "│    └─Sequential: 2-8                             [11, 2048, 20, 30]        --\n",
       "│    │    └─Bottleneck: 3-14                       [11, 2048, 20, 30]        6,039,552\n",
       "│    │    └─Bottleneck: 3-15                       [11, 2048, 20, 30]        4,462,592\n",
       "│    │    └─Bottleneck: 3-16                       [11, 2048, 20, 30]        4,462,592\n",
       "├─Predictor: 1-2                                   [1, 2048, 20, 30]         3,379,200\n",
       "│    └─Linear: 2-9                                 [1, 6600, 512]            1,049,088\n",
       "│    └─Sequential: 2-10                            [1, 6600, 512]            --\n",
       "│    │    └─TransformerBlock: 3-17                 [1, 6600, 512]            3,150,848\n",
       "│    │    └─TransformerBlock: 3-18                 [1, 6600, 512]            3,150,848\n",
       "│    └─Linear: 2-11                                [1, 6600, 2048]           1,050,624\n",
       "│    └─ReLU: 2-12                                  [1, 6600, 2048]           --\n",
       "│    └─Linear: 2-13                                [1, 2048, 600]            3,960,600\n",
       "├─Sequential: 1-3                                  [1, 1024, 20, 30, 1]      --\n",
       "│    └─Linear: 2-14                                [1, 1024, 20, 30, 256]    3,072\n",
       "│    └─ReLU: 2-15                                  [1, 1024, 20, 30, 256]    --\n",
       "│    └─Linear: 2-16                                [1, 1024, 20, 30, 1]      257\n",
       "├─FCNHead: 1-4                                     [1, 49, 20, 30]           --\n",
       "│    └─Conv2d: 2-17                                [1, 512, 20, 30]          9,437,184\n",
       "│    └─BatchNorm2d: 2-18                           [1, 512, 20, 30]          1,024\n",
       "│    └─ReLU: 2-19                                  [1, 512, 20, 30]          --\n",
       "│    └─Dropout: 2-20                               [1, 512, 20, 30]          --\n",
       "│    └─Conv2d: 2-21                                [1, 49, 20, 30]           25,137\n",
       "├─FCNHead: 1-5                                     [1, 49, 20, 30]           --\n",
       "│    └─Conv2d: 2-22                                [1, 256, 20, 30]          2,359,296\n",
       "│    └─BatchNorm2d: 2-23                           [1, 256, 20, 30]          512\n",
       "│    └─ReLU: 2-24                                  [1, 256, 20, 30]          --\n",
       "│    └─Dropout: 2-25                               [1, 256, 20, 30]          --\n",
       "│    └─Conv2d: 2-26                                [1, 49, 20, 30]           12,593\n",
       "├─ConvTranspose2d: 1-6                             [1, 49, 160, 240]         614,705\n",
       "├─ConvTranspose2d: 1-7                             [1, 49, 160, 240]         614,705\n",
       "====================================================================================================\n",
       "Total params: 52,317,725\n",
       "Trainable params: 52,317,725\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 214.92\n",
       "====================================================================================================\n",
       "Input size (MB): 5.07\n",
       "Forward/backward pass size (MB): 5433.61\n",
       "Params size (MB): 195.75\n",
       "Estimated Total Size (MB): 5634.43\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(11, 3, 160, 240))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import SegmentationTrainTransform\n",
    "from data import ImagesToMaskDataset\n",
    "\n",
    "data_dir = '/Users/louis/Files/deep-learning/Deep-Learning-VQA/dataset'\n",
    "\n",
    "transform_train = SegmentationTrainTransform()\n",
    "dataset_train = ImagesToMaskDataset(os.path.join(data_dir, 'train'), transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3, 160, 240])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 11, 3, 160, 240]) torch.Size([8, 160, 240])\n"
     ]
    }
   ],
   "source": [
    "imgs, targets = next(iter(loader_train))\n",
    "\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
